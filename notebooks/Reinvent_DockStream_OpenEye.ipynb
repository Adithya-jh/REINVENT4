{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `REINVENT 4.4`: Reinforcement Learning with DockStream and OpenEye (docking)\n",
    "\n",
    "\n",
    "This is a simple example of running `Reinvent` with only 1 score component (`DockStream`) using OpenEye applications. To execute this notebook, make sure you have cloned the `DockStream` repository from GitHub and installed the conda environment.\n",
    "You also need OpenEye applications installed with binaries on PATH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the paths\n",
    "_Please update only the USER EDITABLE SECTION such that it reflects your system's installation and execute it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import toml\n",
    "\n",
    "\n",
    "# ===== USER EDITABLE SECTION =====\n",
    "# Edit these paths if your DockStream installation is different\n",
    "dockstream_path = os.path.expanduser(\"/path/to/DockStream\")\n",
    "dockstream_env = os.path.expanduser(\"/path/to/miniforge3/envs/DockStream\")\n",
    "\n",
    "# Edit these file names for your specific protein target\n",
    "apo_protein_filename = \"apo_7xn1.pdb\"  # Change this to your apo protein file name\n",
    "reference_ligand_filename = \"tacrine_7xn1.pdb\"  # Change this to your reference ligand file name\n",
    "\n",
    "# Set environment variables for OE applications and OE license\n",
    "os.environ['PATH'] = \"/path/to/openeye/bin:\" + os.environ.get('PATH', '')\n",
    "os.environ['OE_LICENSE'] = \"/path/to/oe_license.txt\"\n",
    "\n",
    "# Change this prefix for your specific protein target\n",
    "output_prefix = \"p7xn1\"\n",
    "\n",
    "# Update project folder path\n",
    "project_dir = os.path.expanduser(\"/path/to/project/folder\")\n",
    "# ===== END OF USER EDITABLE SECTION =====\n",
    "\n",
    "\n",
    "# Update paths for entry points\n",
    "target_preparator = os.path.join(dockstream_path, \"target_preparator.py\")\n",
    "docker = os.path.join(dockstream_path, \"docker.py\")\n",
    "\n",
    "input_data_dir = os.path.join(project_dir, \"input_data\")\n",
    "output_data_dir = os.path.join(project_dir, \"output_data\")\n",
    "logs_dir = os.path.join(project_dir, \"logs\")\n",
    "config_dir = os.path.join(project_dir, \"configs\")\n",
    "lig_docked_dir = os.path.join(output_data_dir, \"ligands_docked\")\n",
    "scores_dir = os.path.join(output_data_dir, \"docking_scores\")\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [input_data_dir, output_data_dir, logs_dir, config_dir, lig_docked_dir, scores_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Update file paths\n",
    "apo_protein_path = os.path.join(input_data_dir, apo_protein_filename)\n",
    "reference_ligand_path = os.path.join(input_data_dir, reference_ligand_filename)\n",
    "\n",
    "target_prep_path = os.path.join(config_dir, f\"{output_prefix}_target_prep.json\")\n",
    "fixed_pdb_path = os.path.join(input_data_dir, f\"{output_prefix}_fixed_target.pdb\")\n",
    "receptor_path = os.path.join(input_data_dir, f\"{output_prefix}_receptor.oeb\")\n",
    "receptor_path_oedu = os.path.join(input_data_dir, f\"{output_prefix}_receptor.oedu\")\n",
    "log_file_target_prep = os.path.join(logs_dir, f\"{output_prefix}_target_prep.log\")\n",
    "log_file_docking = os.path.join(logs_dir, f\"{output_prefix}_docking.log\")\n",
    "log_file_reinvent = os.path.join(logs_dir, f\"{output_prefix}_reinvent.log\")\n",
    "\n",
    "docking_path = os.path.join(config_dir, f\"{output_prefix}_docking.json\")\n",
    "ligands_docked_path = os.path.join(lig_docked_dir, f\"{output_prefix}_ligands_docked.sdf\")\n",
    "ligands_scores_path = os.path.join(scores_dir, f\"{output_prefix}_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up the Target Preparation JSON Configuration\n",
    "Update the PDBFixer block if needed. Other cavity definition methods can be found at https://github.com/MolecularAI/DockStream/blob/c62e6abd919b5b54d144f5f792d40663c9a43a5b/examples/target_preparation/OpenEye_target_preparation.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_dict = {\n",
    "  \"target_preparation\":\n",
    "  {\n",
    "    \"header\": {                                   # general settings\n",
    "      \"environment\": {\n",
    "      },\n",
    "      \"logging\": {                                # logging settings (e.g. which file to write to)\n",
    "        \"logfile\": log_file_target_prep\n",
    "      }\n",
    "    },\n",
    "    \"input_path\": apo_protein_path,                  # this should be an absolute path\n",
    "    \"fixer\": {                                    # based on \"PDBFixer\"; tries to fix common problems with PDB files\n",
    "      \"enabled\": True,\n",
    "      \"standardize\": True,                        # enables standardization of residues\n",
    "      \"remove_heterogens\": True,                  # remove hetero-entries\n",
    "      \"fix_missing_heavy_atoms\": True,            # if possible, fix missing heavy atoms\n",
    "      \"fix_missing_hydrogens\": True,              # add hydrogens, which are usually not present in PDB files\n",
    "      \"fix_missing_loops\": False,                 # add missing loops; CAUTION: the result is usually not sufficient\n",
    "      \"add_water_box\": False,                     # if you want to put the receptor into a box of water molecules\n",
    "      \"fixed_pdb_path\": fixed_pdb_path            # if specified and not \"None\", the fixed PDB file will be stored here\n",
    "    },\n",
    "    \"runs\": [                                     # \"runs\" holds a list of backend runs; at least one is required\n",
    "      {\n",
    "        \"backend\": \"OpenEye\",                # one of the backends supported (\"AutoDockVina\", \"OpenEye\", ...)\n",
    "        \"output\": {\n",
    "          \"receptor_path\": receptor_path      # the generated receptor file will be saved to this location\n",
    "        },\n",
    "        \"parameters\": {},\n",
    "        \"cavity\": {                               # there are different ways to specify the cavity; here, a reference\n",
    "                                                  # ligand is used\n",
    "          \"method\": \"reference_ligand\",\n",
    "          \"reference_ligand_path\": reference_ligand_path,\n",
    "          \"reference_ligand_format\": \"pdb\"\n",
    "}}]}}\n",
    "\n",
    "with open(target_prep_path, 'w') as f:\n",
    "    json.dump(tp_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Target Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{dockstream_env}/bin/python {target_preparator} -conf {target_prep_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Receptor OEDU file\n",
    "New file format used by 2024+ OE apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!oeb2dureceptor -in {receptor_path} -out {receptor_path_oedu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set up the Docking JSON Configuration\n",
    "Update docking run parameters if needed. Different OE scoring functions can be found at https://docs.eyesopen.com/toolkits/cpp/dockingtk/scoring.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_dict = {\n",
    "  \"docking\": {\n",
    "    \"header\": {\n",
    "      \"environment\": {\n",
    "      },\n",
    "      \"logging\": {\n",
    "        \"logfile\": log_file_docking\n",
    "      }\n",
    "    },\n",
    "    \"ligand_preparation\": {\n",
    "      \"embedding_pools\": [\n",
    "        {\n",
    "          \"pool_id\": \"Omega_pool\",\n",
    "          \"type\": \"Omega\",\n",
    "          \"parameters\": {\n",
    "            \"mode\": \"classic\"\n",
    "          },\n",
    "          \"input\": {\n",
    "            \"standardize_smiles\": False,\n",
    "            \"type\": \"console\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"docking_runs\": [\n",
    "      {\n",
    "        \"backend\": \"Hybrid\",\n",
    "        \"run_id\": \"Hybrid\",\n",
    "        \"input_pools\": [\n",
    "          \"Omega_pool\"\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "          \"seed\": 42,\n",
    "          \"receptor_paths\": [\n",
    "            receptor_path_oedu\n",
    "          ],\n",
    "          \"scoring\": \"Chemgauss4\",\n",
    "          \"resolution\": \"High\",\n",
    "          \"number_poses\": 3\n",
    "        },\n",
    "        \"output\": {\n",
    "          \"poses\": {\n",
    "            \"poses_path\": ligands_docked_path,\n",
    "            \"overwrite\": False\n",
    "          },\n",
    "          \"scores\": {\n",
    "            \"scores_path\": ligands_scores_path,\n",
    "            \"overwrite\": False\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(docking_path, 'w+') as f:\n",
    "    json.dump(ed_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set up Reinvent .toml configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "toml_string = f\"\"\"\n",
    "# REINVENT4 TOML input example for reinforcement/curriculum learning\n",
    "#\n",
    "#\n",
    "# Curriculum learning in REINVENT4 is a multi-stage reinforcement learning\n",
    "# run.  One or more stages (auto CL) can be defined.  But it is also\n",
    "# possible to continue a run from any checkpoint file that is generated\n",
    "# during the run (manual CL).  Currently checkpoints are written at the end\n",
    "# of a run also when the run is forcefully terminated with Ctrl-C.\n",
    "\n",
    "\n",
    "run_type = \"staged_learning\"\n",
    "device = \"cuda:0\"  # Edit this if you want to use a different device\n",
    "tb_logdir = \"/tb_logs\"  # Relative path to the TensorBoard logs directory  # Edit this path as needed\n",
    "#json_out_config = \"_staged_learning.json\"  # write this TOML to JSON\n",
    "\n",
    "[parameters]\n",
    "\n",
    "# Uncomment one of the comment blocks below.  Each generator needs a model\n",
    "# file and possibly a SMILES file with seed structures.  If the run is to\n",
    "# be continued after termination, the agent_file would have to be replaced\n",
    "# with the checkpoint file.\n",
    "\n",
    "\n",
    "use_checkpoint = true  # if true read diversity filter from agent_file\n",
    "purge_memories = false  # if true purge all diversity filter memories after each stage\n",
    "\n",
    "## Reinvent\n",
    "prior_file = \"/path/to/reinvent.model.chkpt\" # use checkpoint files or priors\n",
    "agent_file = \"/path/to/reinvent.model.chkpt\"\n",
    "\n",
    "## LibInvent\n",
    "#prior_file = \"priors/libinvent.prior\"\n",
    "#agent_file = \"priors/libinvent.prior\"\n",
    "#smiles_file = \"scaffolds.smi\"  # 1 scaffold per line with attachment points\n",
    "\n",
    "## LinkInvent\n",
    "#prior_file = \"priors/linkinvent.prior\"\n",
    "#agent_file = \"priors/linkinvent.prior\"\n",
    "#smiles_file = \"warheads.smi\"  # 2 warheads per line separated with '|'\n",
    "\n",
    "## Mol2Mol\n",
    "#prior_file = \"priors/mol2mol_scaffold_generic.prior\"\n",
    "#agent_file = \"priors/mol2mol_scaffold_generic.prior\"\n",
    "#smiles_file = \"mol2mol.smi\"  # 1 compound per line\n",
    "#sample_strategy = \"multinomial\"  # multinomial or beamsearch (deterministic)\n",
    "#distance_threshold = 100\n",
    "\n",
    "batch_size = 128          # network\n",
    "\n",
    "unique_sequences = true  # if true remove all duplicates raw sequences in each step\n",
    "                         # only here for backward compatibility\n",
    "randomize_smiles = true  # if true shuffle atoms in SMILES randomly\n",
    "\n",
    "\n",
    "[learning_strategy]\n",
    "\n",
    "type = \"dap\"      # dap: only one supported\n",
    "sigma = 128       # sigma of the RL reward function\n",
    "rate = 0.0001     # for torch.optim\n",
    "\n",
    "\n",
    "[diversity_filter]  # optional, comment section out or remove if unneeded\n",
    "                    # NOTE: also memorizes all seen SMILES\n",
    "\n",
    "type = \"IdenticalMurckoScaffold\" # IdenticalTopologicalScaffold,\n",
    "                                 # ScaffoldSimilarity, PenalizeSameSmiles\n",
    "bucket_size = 25                 # memory size in number of compounds\n",
    "minscore = 0.4                   # only memorize if this threshold is exceeded\n",
    "minsimilarity = 0.4              # minimum similarity for ScaffoldSimilarity\n",
    "penalty_multiplier = 0.5         # penalty factor for PenalizeSameSmiles\n",
    "\n",
    "\n",
    "# Reinvent only: guide RL in the initial phase\n",
    "#[inception]  # optional, comment sectionout or remove if unneeded\n",
    "\n",
    "#smiles_file = \"sampled.smi\"  # \"good\" SMILES for guidance\n",
    "#memory_size = 100  # number of total SMILES held in memory\n",
    "#sample_size = 10  # number of SMILES randomly chosen each epoch\n",
    "\n",
    "\n",
    "### Stage 1\n",
    "### Note that stages must always be a list i.e. double brackets\n",
    "[[stage]]\n",
    "\n",
    "chkpt_file = '/path/to/rl_run.chkpt'  # Edit this checkpoint file path\n",
    "termination = \"simple\"  # termination criterion fot this stage\n",
    "max_score = 0.6  # terminate if this total score is exceeded\n",
    "min_steps = 25  # run for at least this number of steps\n",
    "max_steps = 1000  # terminate entire run when exceeded\n",
    "\n",
    "# Optionally, a DF can be set for each stage but note that the global DF\n",
    "# section above will always overwrite the stage section and you need to\n",
    "# delete [diversity_filter] to avoid this\n",
    "#\n",
    "#[stage.diversity_filter]\n",
    "#type = \"IdenticalMurckoScaffold\"\n",
    "# etc.\n",
    "\n",
    "[stage.scoring]\n",
    "type = \"geometric_mean\"  # aggregation function\n",
    "\n",
    "[[stage.scoring.component]]\n",
    "[[stage.scoring.component.DockStream.endpoint]]\n",
    "name = \"Docking\"\n",
    "weight = 1\n",
    "\n",
    "params.configuration_path = {docking_path}\n",
    "params.docker_script_path = {docker}\n",
    "params.docker_python_path =  \"{dockstream_env}/bin/python\"\n",
    "transform.type = \"reverse_sigmoid\"\n",
    "transform.high = -8\n",
    "transform.low = -16\n",
    "transform.k = 0.25\n",
    "\n",
    "### Stage 2\n",
    "### next stage if wanted\n",
    "# [[stage]]\n",
    "# ...\n",
    "\"\"\"\n",
    "\n",
    "# Define the output path\n",
    "toml_path = os.path.join(project_dir, \"config.toml\")\n",
    "\n",
    "# Parse the TOML string\n",
    "toml_dict = toml.loads(toml_string)\n",
    "\n",
    "# Write the TOML content to a file\n",
    "with open(toml_path, 'w') as f:\n",
    "    toml.dump(toml_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run REINVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!reinvent -l {log_file_reinvent} {toml_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse the Results\n",
    "\n",
    "To analyze the run using TensorBoard:\n",
    "1. Open a terminal and navigate to the project directory.\n",
    "2. Run the command: `tensorboard --logdir=tb_logs`\n",
    "3. Open the provided URL (usually http://localhost:6006) in your web browser to view the TensorBoard dashboard.\n",
    "\n",
    "All docking scores are saved inside `summary_1.csv` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
